{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":12698823,"datasetId":8025428,"databundleVersionId":13310129}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Environment Initialization and Dataset Verification\n<p style=\"font-size:18px; font-weight:normal;\">\n This cell imports essential libraries (NumPy, Pandas, os) and verifies that the dataset is correctly mounted in the Kaggle environment.\n It lists all files under /kaggle/input to ensure that video data is accessible before model training.\n </p>","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-15T11:08:29.587444Z","iopub.execute_input":"2026-02-15T11:08:29.58767Z","iopub.status.idle":"2026-02-15T11:08:36.03857Z","shell.execute_reply.started":"2026-02-15T11:08:29.587648Z","shell.execute_reply":"2026-02-15T11:08:36.037899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision\nimport timm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T11:08:36.039959Z","iopub.execute_input":"2026-02-15T11:08:36.0405Z","iopub.status.idle":"2026-02-15T11:08:47.278949Z","shell.execute_reply.started":"2026-02-15T11:08:36.040476Z","shell.execute_reply":"2026-02-15T11:08:47.27817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(torch.cuda.is_available())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T11:08:47.279886Z","iopub.execute_input":"2026-02-15T11:08:47.280179Z","iopub.status.idle":"2026-02-15T11:08:47.525719Z","shell.execute_reply.started":"2026-02-15T11:08:47.280151Z","shell.execute_reply":"2026-02-15T11:08:47.524838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install dependencies\n\n!pip -q install timm decord grad-cam --upgrade","metadata":{"execution":{"iopub.status.busy":"2026-02-15T11:08:47.526711Z","iopub.execute_input":"2026-02-15T11:08:47.526986Z","iopub.status.idle":"2026-02-15T11:08:59.451586Z","shell.execute_reply.started":"2026-02-15T11:08:47.526954Z","shell.execute_reply":"2026-02-15T11:08:59.450612Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports & Reproducibility Setup\n\n<p style=\"font-size:18px; font-weight:normal;\">\nThis cell imports all required libraries for deep learning, video and audio processing, and model evaluation.  \nIt also sets a fixed random seed to ensure reproducibility and defines the computation device (CPU/GPU) for training.\n</p>","metadata":{}},{"cell_type":"code","source":"#  Imports & Seed\n\nimport os, glob, re, random, math, time, hashlib, subprocess\nimport numpy as np\nimport pandas as pd\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\n\nfrom decord import VideoReader, cpu\nimport timm\nimport torchaudio\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n\nimport matplotlib.pyplot as plt\n\nSEED = 42\ndef seed_everything(seed=SEED):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nseed_everything()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T11:08:59.453877Z","iopub.execute_input":"2026-02-15T11:08:59.454131Z","iopub.status.idle":"2026-02-15T11:09:01.008474Z","shell.execute_reply.started":"2026-02-15T11:08:59.454104Z","shell.execute_reply":"2026-02-15T11:09:01.007728Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Configuration Settings\n\n<p style=\"font-size:18px; font-weight:normal;\">\nThis section defines all global hyperparameters and training settings used throughout the project, including image size, number of frames, learning rates, batch size, and audio parameters.  \nIt centralizes configuration values to ensure consistency, reproducibility, and easy experimentation.\n</p>","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass CFG:\n    img_size: int = 224\n    k_frames: int = 16\n    batch_size: int = 4\n    num_workers: int = 2\n    epochs_baseline: int = 4\n    epochs_av: int = 6\n    lr_baseline: float = 3e-4\n    lr_av: float = 2e-4\n    weight_decay: float = 1e-4\n    patience: int = 2\n    sample_rate: int = 16000\n    audio_seconds: int = 4\n    n_mfcc: int = 40\n    audio_cache_dir: str = \"/kaggle/working/audio_cache\"\n    work_dir: str = \"/kaggle/working\"\n\ncfg = CFG()\nos.makedirs(cfg.audio_cache_dir, exist_ok=True)\n\nMEAN = (0.485, 0.456, 0.406)\nSTD  = (0.229, 0.224, 0.225)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T11:09:01.00951Z","iopub.execute_input":"2026-02-15T11:09:01.009948Z","iopub.status.idle":"2026-02-15T11:09:01.016928Z","shell.execute_reply.started":"2026-02-15T11:09:01.009922Z","shell.execute_reply":"2026-02-15T11:09:01.016258Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Video Sampling and Preprocessing\n\n<p style=\"font-size:18px; font-weight:normal;\">\nThis section defines helper functions for extracting and preprocessing video frames.  \nIt samples a fixed number of frames per video, applies optional data augmentation, and performs center-crop resizing to ensure consistent input dimensions for the model.\n</p>","metadata":{}},{"cell_type":"code","source":"def sample_frame_indices(num_frames_total, k=16, strategy=\"uniform\"):\n    if num_frames_total <= 0:\n        return np.zeros((k,), dtype=np.int64)\n    if strategy == \"uniform\":\n        idx = np.linspace(0, num_frames_total - 1, k).round().astype(np.int64)\n        return idx\n    if strategy == \"random\":\n        if num_frames_total >= k:\n            start = random.randint(0, num_frames_total - k)\n            return np.arange(start, start + k, dtype=np.int64)\n        else:\n            idx = np.arange(num_frames_total, dtype=np.int64)\n            pad = np.full((k - num_frames_total,), num_frames_total - 1, dtype=np.int64)\n            return np.concatenate([idx, pad])\n    raise ValueError(\"Unknown strategy\")\n\ndef random_horizontal_flip(frames, p=0.5):\n    if random.random() < p:\n        return frames[:, :, ::-1, :]\n    return frames\n\ndef center_crop_resize(frames, out_size=224):\n    # frames: (T,H,W,3) uint8\n    import cv2\n    out = []\n    for fr in frames:\n        h, w, _ = fr.shape\n        scale = out_size / min(h, w)\n        nh, nw = int(h * scale), int(w * scale)\n        fr = cv2.resize(fr, (nw, nh), interpolation=cv2.INTER_AREA)\n        h, w, _ = fr.shape\n        y0 = (h - out_size) // 2\n        x0 = (w - out_size) // 2\n        fr = fr[y0:y0 + out_size, x0:x0 + out_size]\n        out.append(fr)\n    return np.stack(out, axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T11:09:01.017823Z","iopub.execute_input":"2026-02-15T11:09:01.018085Z","iopub.status.idle":"2026-02-15T11:09:01.033722Z","shell.execute_reply.started":"2026-02-15T11:09:01.018051Z","shell.execute_reply":"2026-02-15T11:09:01.032981Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Audio Feature Extraction (MFCC)\n\n<p style=\"font-size:18px; font-weight:normal;\">\nThis section extracts audio features from each video using MFCC representations.  \nAudio is converted to mono 16kHz format, standardized in length, and transformed into MFCC coefficients. If no audio is available, a zero tensor is returned to maintain consistent input dimensions.\n</p>","metadata":{}},{"cell_type":"code","source":"SAMPLE_RATE = cfg.sample_rate\nN_MFCC = cfg.n_mfcc\n\nmfcc_tf = torchaudio.transforms.MFCC(\n    sample_rate=SAMPLE_RATE,\n    n_mfcc=N_MFCC,\n    melkwargs={\"n_fft\": 400, \"hop_length\": 160, \"n_mels\": 64, \"center\": True},\n)\n\ndef audio_path_for_video(video_path: str):\n    h = hashlib.md5(video_path.encode(\"utf-8\")).hexdigest()\n    return os.path.join(cfg.audio_cache_dir, f\"{h}.wav\")\n\ndef ensure_audio(video_path: str):\n    out = audio_path_for_video(video_path)\n    if os.path.exists(out) and os.path.getsize(out) > 1000:\n        return out\n    # extract audio from mp4 -> wav mono 16k\n    cmd = [\n    \"ffmpeg\",\n    \"-y\",\n    \"-loglevel\", \"quiet\",\n    \"-i\", video_path,\n    \"-vn\",\n    \"-ac\", \"1\",\n    \"-ar\", str(SAMPLE_RATE),\n    out\n]\n\n    try:\n        subprocess.run(cmd, check=False, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n\n        if os.path.exists(out) and os.path.getsize(out) > 1000:\n            return out\n    except Exception:\n        pass\n    return None  # if no audio or ffmpeg failed\n\ndef load_mfcc_or_zeros(video_path: str):\n    apath = ensure_audio(video_path)\n    if apath is None:\n        # Return consistent shape zeros (n_mfcc, time)\n        return torch.zeros((N_MFCC, 401), dtype=torch.float32)\n\n    wav, sr = torchaudio.load(apath)\n    wav = wav.mean(dim=0, keepdim=True)  # mono\n    if sr != SAMPLE_RATE:\n        wav = torchaudio.functional.resample(wav, sr, SAMPLE_RATE)\n\n    target_len = SAMPLE_RATE * cfg.audio_seconds\n    if wav.shape[1] >= target_len:\n        wav = wav[:, :target_len]\n    else:\n        wav = F.pad(wav, (0, target_len - wav.shape[1]))\n\n    mfcc = mfcc_tf(wav)          # (1, n_mfcc, time)\n    mfcc = torch.log1p(mfcc)\n    return mfcc.squeeze(0)       # (n_mfcc, time)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T11:09:01.034628Z","iopub.execute_input":"2026-02-15T11:09:01.034901Z","iopub.status.idle":"2026-02-15T11:09:01.08119Z","shell.execute_reply.started":"2026-02-15T11:09:01.034866Z","shell.execute_reply":"2026-02-15T11:09:01.080639Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset Discovery and Automatic Label Inference\n\n<p style=\"font-size:18px; font-weight:normal;\">\nThis section scans the FaceForensics++ dataset to locate all video files and automatically assign binary labels based on directory naming conventions.  \nVideos labeled as \"original/pristine\" are assigned class 0 (real), while manipulated methods (e.g., DeepFakes, FaceSwap) are assigned class 1 (fake).\n</p>","metadata":{}},{"cell_type":"code","source":"def find_all_mp4(base=\"/kaggle/input\"):\n    mp4s = glob.glob(base + \"/**/*.mp4\", recursive=True)\n    print(\"Found mp4:\", len(mp4s))\n    return mp4s\n\nmp4s = find_all_mp4()\n\n# Label inference: FaceForensics++ usually has \"original\" vs manipulated methods\n# We'll treat:\n#  - Original / pristine / real => label 0\n#  - Anything under manipulated methods (deepfakes, face2face, faceswap, neuraltextures, etc) => label 1\nFAKE_KEYS = [\n    \"deepfakes\", \"face2face\", \"faceswap\", \"neuraltextures\", \"manipulated\",\n    \"fake\", \"df\", \"f2f\", \"fs\", \"nt\"\n]\nREAL_KEYS = [\"original\", \"pristine\", \"real\"]\n\ndef infer_label_ffpp(path: str):\n    s = path.lower()\n    if any(k in s for k in REAL_KEYS) and not any(k in s for k in FAKE_KEYS):\n        return 0\n    if any(k in s for k in FAKE_KEYS) and not any(k in s for k in REAL_KEYS):\n        return 1\n    # If ambiguous, we try a stronger rule:\n    # if contains \"original\" anywhere -> real\n    if \"original\" in s or \"pristine\" in s:\n        return 0\n    # otherwise if contains known manip methods -> fake\n    if any(k in s for k in [\"deepfakes\", \"face2face\", \"faceswap\", \"neuraltextures\"]):\n        return 1\n    return None\n\nrows = []\nfor p in mp4s:\n    y = infer_label_ffpp(p)\n    if y is not None:\n        rows.append({\"video_path\": p, \"label\": y})\n\ndf = pd.DataFrame(rows)\nprint(\"Labeled videos:\", len(df), \"out of\", len(mp4s))\nif len(df) == 0:\n    raise RuntimeError(\"Could not infer labels. Print mp4 paths and adjust infer_label_ffpp().\")\n\nprint(\"Label distribution:\", df[\"label\"].value_counts().to_dict())\nprint(df.sample(5, random_state=SEED))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T11:09:01.081966Z","iopub.execute_input":"2026-02-15T11:09:01.082165Z","iopub.status.idle":"2026-02-15T11:09:02.166171Z","shell.execute_reply.started":"2026-02-15T11:09:01.082145Z","shell.execute_reply":"2026-02-15T11:09:02.16543Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train, Validation, and Test Split\n\n<p style=\"font-size:18px; font-weight:normal;\">\nThis section removes duplicate entries and performs a stratified split of the dataset into training (70%), validation (15%), and test (15%) sets.  \nStratification ensures balanced class distribution across all subsets, and the splits are saved as CSV files for reproducibility.\n</p>","metadata":{}},{"cell_type":"code","source":"df = df.drop_duplicates(subset=[\"video_path\"]).reset_index(drop=True)\n\n# Stratified split\ntrain_df, tmp_df = train_test_split(df, test_size=0.30, random_state=SEED, stratify=df[\"label\"])\nval_df, test_df  = train_test_split(tmp_df, test_size=0.50, random_state=SEED, stratify=tmp_df[\"label\"])\n\ntrain_csv = os.path.join(cfg.work_dir, \"train.csv\")\nval_csv   = os.path.join(cfg.work_dir, \"val.csv\")\ntest_csv  = os.path.join(cfg.work_dir, \"test.csv\")\n\ntrain_df.to_csv(train_csv, index=False)\nval_df.to_csv(val_csv, index=False)\ntest_df.to_csv(test_csv, index=False)\n\nprint(\"Saved:\", train_csv, val_csv, test_csv)\nprint(\"Train:\", train_df[\"label\"].value_counts().to_dict())\nprint(\"Val  :\", val_df[\"label\"].value_counts().to_dict())\nprint(\"Test :\", test_df[\"label\"].value_counts().to_dict())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T11:09:02.167163Z","iopub.execute_input":"2026-02-15T11:09:02.167657Z","iopub.status.idle":"2026-02-15T11:09:02.201066Z","shell.execute_reply.started":"2026-02-15T11:09:02.167634Z","shell.execute_reply":"2026-02-15T11:09:02.200531Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Custom Dataset and DataLoaders\n\n<p style=\"font-size:18px; font-weight:normal;\">\nThis section defines a custom PyTorch Dataset that loads video frames and corresponding audio features for each sample.  \nVideo frames are sampled, augmented (during training), normalized, and converted to tensors, while MFCC audio features are extracted and aligned. DataLoaders are then created for efficient batch processing.\n</p>","metadata":{}},{"cell_type":"code","source":"class RealVisionDataset(Dataset):\n    def __init__(self, csv_path, k_frames=16, train=True):\n        self.df = pd.read_csv(csv_path)\n        self.k_frames = k_frames\n        self.train = train\n        assert set(self.df[\"label\"].unique()).issubset({0,1})\n\n    def _load_video_frames(self, video_path):\n        vr = VideoReader(video_path, ctx=cpu(0))\n        n = len(vr)\n        idx = sample_frame_indices(n, k=self.k_frames, strategy=\"uniform\")\n        frames = vr.get_batch(idx).asnumpy()  # (T,H,W,3)\n\n        if self.train:\n            frames = random_horizontal_flip(frames, p=0.5)\n\n        frames = center_crop_resize(frames, out_size=cfg.img_size)\n        x = torch.from_numpy(frames).float() / 255.0\n        x = x.permute(0,3,1,2)  # (T,3,H,W)\n        # --- ImageNet normalization ---\n        mean = torch.tensor(MEAN).view(1,3,1,1)\n        std  = torch.tensor(STD).view(1,3,1,1)\n        x = (x - mean) / std\n        return x\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, i):\n        row = self.df.iloc[i]\n        vpath = row[\"video_path\"]\n        y = int(row[\"label\"])\n\n        v = self._load_video_frames(vpath)\n        a = load_mfcc_or_zeros(vpath)  # (n_mfcc, time)\n        return v, a, torch.tensor(y, dtype=torch.long)\n\ntrain_ds = RealVisionDataset(train_csv, k_frames=cfg.k_frames, train=True)\nval_ds   = RealVisionDataset(val_csv,   k_frames=cfg.k_frames, train=False)\ntest_ds  = RealVisionDataset(test_csv,  k_frames=cfg.k_frames, train=False)\n\ntrain_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n                          num_workers=cfg.num_workers, pin_memory=True, drop_last=True)\nval_loader   = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False,\n                          num_workers=cfg.num_workers, pin_memory=True)\ntest_loader  = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False,\n                          num_workers=cfg.num_workers, pin_memory=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T11:09:02.202528Z","iopub.execute_input":"2026-02-15T11:09:02.202747Z","iopub.status.idle":"2026-02-15T11:09:02.220293Z","shell.execute_reply.started":"2026-02-15T11:09:02.202727Z","shell.execute_reply":"2026-02-15T11:09:02.219795Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Sanity Checks\n\n<p style=\"font-size:18px; font-weight:normal;\">\nThis section performs basic sanity checks to verify correct data loading before training.  \nIt confirms balanced class distribution, validates input tensor shapes (video and audio), and prints a sample video path to ensure dataset integrity.\n</p>","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nprint(\"Train label counts:\", Counter(train_ds.df[\"label\"].tolist()))\n\nv, a, y = train_ds[0]\nprint(\"Sample shapes:\", v.shape, a.shape, y.item())\nprint(\"First video path:\", train_ds.df.iloc[0][\"video_path\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T11:09:02.221401Z","iopub.execute_input":"2026-02-15T11:09:02.221701Z","iopub.status.idle":"2026-02-15T11:09:03.092504Z","shell.execute_reply.started":"2026-02-15T11:09:02.221669Z","shell.execute_reply":"2026-02-15T11:09:03.09184Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Architectures\n\n<p style=\"font-size:18px; font-weight:normal;\">\nThis section defines the model architectures used in the project.  \nA video-only baseline model is implemented using a pre-trained EfficientNet backbone, while the main multi-modal model integrates spatial video features, temporal modeling via a Transformer encoder, and audio embeddings through MFCC-based convolutional layers.\n</p>","metadata":{}},{"cell_type":"code","source":"class VideoOnlyBaseline(nn.Module):\n    def __init__(self, backbone=\"tf_efficientnet_b0\", emb_dim=256):\n        super().__init__()\n        self.cnn = timm.create_model(backbone, pretrained=True, num_classes=0, global_pool=\"\")\n        ch = self.cnn.num_features\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.proj = nn.Linear(ch, emb_dim)\n        self.classifier = nn.Linear(emb_dim, 2)\n\n    def forward(self, v):  # (B,T,3,H,W)\n        B,T,C,H,W = v.shape\n        x = v.view(B*T, C, H, W)\n        feat = self.cnn(x)\n        if feat.dim() == 4:\n            feat = self.pool(feat).flatten(1)\n        feat = feat.view(B, T, -1).mean(dim=1)\n        feat = self.proj(feat)\n        return self.classifier(feat)\n\nclass AudioEncoder(nn.Module):\n    def __init__(self, n_mfcc=40, emb_dim=256):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv1d(n_mfcc, 128, kernel_size=5, padding=2),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.MaxPool1d(2),\n\n            nn.Conv1d(128, 256, kernel_size=5, padding=2),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool1d(1),\n        )\n        self.proj = nn.Linear(256, emb_dim)\n\n    def forward(self, a):  # (B, n_mfcc, time)\n        x = self.net(a).squeeze(-1)      # (B,256)\n        return self.proj(x)\n\nclass RealVisionAV(nn.Module):\n    def __init__(self, backbone=\"tf_efficientnet_b0\", v_emb=256, a_emb=256, nheads=4, nlayers=2):\n        super().__init__()\n        self.cnn = timm.create_model(backbone, pretrained=True, num_classes=0, global_pool=\"\")\n        ch = self.cnn.num_features\n        self.v_pool = nn.AdaptiveAvgPool2d(1)\n        self.v_proj = nn.Linear(ch, v_emb)\n\n        enc_layer = nn.TransformerEncoderLayer(d_model=v_emb, nhead=nheads, batch_first=True)\n        self.temporal = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n\n        self.audio = AudioEncoder(n_mfcc=N_MFCC, emb_dim=a_emb)\n\n        self.fusion = nn.Sequential(\n            nn.Linear(v_emb + a_emb, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 2)\n        )\n\n    def forward(self, v, a):\n        # v: (B,T,3,H,W), a: (B,n_mfcc,time)\n        B,T,C,H,W = v.shape\n        x = v.view(B*T, C, H, W)\n        feat = self.cnn(x)\n        if feat.dim() == 4:\n            feat = self.v_pool(feat).flatten(1)\n        feat = feat.view(B, T, -1)\n        feat = self.v_proj(feat)          # (B,T,v_emb)\n\n        feat = self.temporal(feat)        # (B,T,v_emb)\n        v_emb = feat.mean(dim=1)          # (B,v_emb)\n        a_emb = self.audio(a)             # (B,a_emb)\n\n        alpha = 0.3\n        fused = torch.cat([v_emb, a_emb], dim=1)\n        return self.fusion(fused)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T11:09:03.093427Z","iopub.execute_input":"2026-02-15T11:09:03.093686Z","iopub.status.idle":"2026-02-15T11:09:03.105881Z","shell.execute_reply.started":"2026-02-15T11:09:03.093664Z","shell.execute_reply":"2026-02-15T11:09:03.105199Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Strategy and Evaluation Metrics\n\n<p style=\"font-size:18px; font-weight:normal;\">\nThis section defines the training loop, evaluation metrics (Accuracy, F1-score, AUC), and optimization strategy.  \nThe model is trained using weighted cross-entropy to address potential class imbalance, AdamW optimization, cosine learning rate scheduling, and early stopping based on validation F1-score.\n</p>","metadata":{}},{"cell_type":"code","source":"def compute_metrics(y_true, y_prob):\n    y_pred = (y_prob[:,1] >= 0.5).astype(int)\n    acc = accuracy_score(y_true, y_pred)\n    f1  = f1_score(y_true, y_pred)\n    try:\n        auc = roc_auc_score(y_true, y_prob[:,1])\n    except:\n        auc = float(\"nan\")\n    return acc, f1, auc\n\ndef get_class_weights_from_train(train_df):\n    counts = train_df[\"label\"].value_counts().to_dict()\n    w0 = 1.0 / max(counts.get(0,1), 1)\n    w1 = 1.0 / max(counts.get(1,1), 1)\n    w = torch.tensor([w0, w1], dtype=torch.float32, device=device)\n    return w\n\ndef train_one_epoch(model, loader, optimizer, scaler, criterion, is_av: bool):\n    model.train()\n    losses, all_y, all_p = [], [], []\n    for v,a,y in loader:\n        v = v.to(device, non_blocking=True)\n        a = a.to(device, non_blocking=True)\n        y = y.to(device, non_blocking=True)\n\n        optimizer.zero_grad(set_to_none=True)\n        with autocast():\n            logits = model(v,a) if is_av else model(v)\n            loss = criterion(logits, y)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        losses.append(loss.item())\n        prob = torch.softmax(logits.detach(), dim=1).float().cpu().numpy()\n        all_p.append(prob)\n        all_y.append(y.detach().cpu().numpy())\n\n    all_p = np.concatenate(all_p, axis=0)\n    all_y = np.concatenate(all_y, axis=0)\n    acc,f1,auc = compute_metrics(all_y, all_p)\n    return float(np.mean(losses)), acc, f1, auc\n\n@torch.no_grad()\ndef validate(model, loader, criterion, is_av: bool):\n    model.eval()\n    losses, all_y, all_p = [], [], []\n    for v,a,y in loader:\n        v = v.to(device, non_blocking=True)\n        a = a.to(device, non_blocking=True)\n        y = y.to(device, non_blocking=True)\n\n        logits = model(v,a) if is_av else model(v)\n        loss = criterion(logits, y)\n\n        losses.append(loss.item())\n        prob = torch.softmax(logits, dim=1).float().cpu().numpy()\n        all_p.append(prob)\n        all_y.append(y.cpu().numpy())\n\n    all_p = np.concatenate(all_p, axis=0)\n    all_y = np.concatenate(all_y, axis=0)\n    acc,f1,auc = compute_metrics(all_y, all_p)\n    return float(np.mean(losses)), acc, f1, auc\n\ndef run_training(model, is_av: bool, epochs: int, lr: float, ckpt_path: str):\n    model = model.to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=cfg.weight_decay)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    scaler = GradScaler()\n\n    weights = get_class_weights_from_train(train_df)\n    criterion = nn.CrossEntropyLoss(weight=weights)\n\n    best_f1 = -1.0\n    bad = 0\n\n    for ep in range(1, epochs + 1):\n        tr = train_one_epoch(model, train_loader, optimizer, scaler, criterion, is_av=is_av)\n        va = validate(model, val_loader, criterion, is_av=is_av)\n        scheduler.step()\n\n        tr_loss,tr_acc,tr_f1,tr_auc = tr\n        va_loss,va_acc,va_f1,va_auc = va\n\n        print(f\"Epoch {ep:02d} | \"\n              f\"TR loss {tr_loss:.4f} acc {tr_acc:.3f} f1 {tr_f1:.3f} auc {tr_auc:.3f} | \"\n              f\"VA loss {va_loss:.4f} acc {va_acc:.3f} f1 {va_f1:.3f} auc {va_auc:.3f}\")\n\n        if va_f1 > best_f1:\n            best_f1 = va_f1\n            bad = 0\n            torch.save({\"model\": model.state_dict()}, ckpt_path)\n            print(\"saved best:\", ckpt_path)\n        else:\n            bad += 1\n            if bad >= cfg.patience:\n                print(\"early stopping\")\n                break\n\n    return ckpt_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T11:09:03.108349Z","iopub.execute_input":"2026-02-15T11:09:03.108622Z","iopub.status.idle":"2026-02-15T11:09:03.125111Z","shell.execute_reply.started":"2026-02-15T11:09:03.1086Z","shell.execute_reply":"2026-02-15T11:09:03.124468Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tiny Overfit Sanity Test\n\n<p style=\"font-size:18px; font-weight:normal;\">\nThis section performs a small overfitting test on a subset of 8 training samples to verify that the model and data pipeline are functioning correctly.  \nIf the model fails to achieve high accuracy on this tiny subset, it may indicate issues with data loading, labeling, or model implementation.\n</p>","metadata":{}},{"cell_type":"code","source":"print(\"\\n=== Tiny Overfit Test (8 samples) ===\")\ntiny_idx = list(range(min(8, len(train_ds))))\ntiny = torch.utils.data.Subset(train_ds, tiny_idx)\ntiny_loader = DataLoader(tiny, batch_size=2, shuffle=True)\n\ntmp_model = RealVisionAV(backbone=\"tf_efficientnet_b0\").to(device)\ntmp_opt = torch.optim.AdamW(tmp_model.parameters(), lr=2e-4)\ntmp_crit = nn.CrossEntropyLoss()\ntmp_scaler = GradScaler()\n\nfor ep in range(1, 6):\n    tmp_model.train()\n    ys, ps, losses = [], [], []\n    for v,a,y in tiny_loader:\n        v,a,y = v.to(device), a.to(device), y.to(device)\n        tmp_opt.zero_grad(set_to_none=True)\n        with autocast():\n            logits = tmp_model(v,a)\n            loss = tmp_crit(logits, y)\n        tmp_scaler.scale(loss).backward()\n        tmp_scaler.step(tmp_opt)\n        tmp_scaler.update()\n        losses.append(loss.item())\n        ys += y.detach().cpu().tolist()\n        ps += logits.argmax(1).detach().cpu().tolist()\n    acc = sum(int(p==t) for p,t in zip(ps,ys)) / len(ys)\n    print(ep, \"loss\", float(np.mean(losses)), \"acc\", acc)\n\nprint(\"If this stays ~0.5, your labels/paths are likely wrong or data is not read correctly.\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T11:09:03.125923Z","iopub.execute_input":"2026-02-15T11:09:03.126186Z","iopub.status.idle":"2026-02-15T11:10:16.621232Z","shell.execute_reply.started":"2026-02-15T11:09:03.126165Z","shell.execute_reply":"2026-02-15T11:10:16.620441Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Baseline Model Training (Video-Only)\n\n<p style=\"font-size:18px; font-weight:normal;\">\nThis section trains the video-only baseline model using spatial features extracted from video frames.  \nThe model serves as a reference point for evaluating the contribution of the multi-modal architecture.\n</p>","metadata":{}},{"cell_type":"code","source":"print(\"\\n=== Train Baseline (Video-only) ===\")\nbaseline = VideoOnlyBaseline(backbone=\"tf_efficientnet_b0\")\nbaseline_ckpt = os.path.join(cfg.work_dir, \"baseline_best.pt\")\nbaseline_ckpt = run_training(baseline, is_av=False, epochs=cfg.epochs_baseline, lr=cfg.lr_baseline, ckpt_path=baseline_ckpt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T11:10:16.622184Z","iopub.execute_input":"2026-02-15T11:10:16.62248Z","iopub.status.idle":"2026-02-15T12:17:36.933781Z","shell.execute_reply.started":"2026-02-15T11:10:16.622453Z","shell.execute_reply":"2026-02-15T12:17:36.932969Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Multi-Modal Model Training (Video + Audio)\n\n<p style=\"font-size:18px; font-weight:normal;\">\nThis section trains the proposed multi-modal model that integrates spatial video features, temporal modeling, and audio representations.  \nThe objective is to evaluate whether combining visual and audio information improves deepfake detection performance compared to the baseline.\n</p>","metadata":{}},{"cell_type":"code","source":"print(\"\\n=== Train Main Model (Video+Audio) ===\")\nav_model = RealVisionAV(backbone=\"tf_efficientnet_b0\", v_emb=256, a_emb=256, nheads=4, nlayers=2)\nav_ckpt = os.path.join(cfg.work_dir, \"av_best.pt\")\nav_ckpt = run_training(av_model, is_av=True, epochs=cfg.epochs_av, lr=cfg.lr_av, ckpt_path=av_ckpt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T12:17:36.935072Z","iopub.execute_input":"2026-02-15T12:17:36.93534Z","iopub.status.idle":"2026-02-15T13:57:38.915363Z","shell.execute_reply.started":"2026-02-15T12:17:36.93531Z","shell.execute_reply":"2026-02-15T13:57:38.914483Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Test Evaluation\n\n<p style=\"font-size:18px; font-weight:normal;\">\nThis section loads the best-performing checkpoints of both models and evaluates them on the held-out test set.  \nFinal performance metrics (Accuracy, F1-score, and AUC) are reported to provide an unbiased comparison between the baseline and the proposed multi-modal model.\n</p>","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef load_best(model, ckpt_path):\n    ckpt = torch.load(ckpt_path, map_location=device)\n    model.load_state_dict(ckpt[\"model\"], strict=True)\n    model.to(device).eval()\n    return model\n\n@torch.no_grad()\ndef eval_on_test(model, loader, is_av: bool):\n    model.eval()\n    all_y, all_p = [], []\n    for v,a,y in loader:\n        v = v.to(device, non_blocking=True)\n        a = a.to(device, non_blocking=True)\n        logits = model(v,a) if is_av else model(v)\n        prob = torch.softmax(logits, dim=1).float().cpu().numpy()\n        all_p.append(prob)\n        all_y.append(y.numpy())\n    all_p = np.concatenate(all_p, axis=0)\n    all_y = np.concatenate(all_y, axis=0)\n    acc,f1,auc = compute_metrics(all_y, all_p)\n    return acc,f1,auc\n\nprint(\"\\n=== Final Test Metrics ===\")\nbaseline2 = load_best(VideoOnlyBaseline(backbone=\"tf_efficientnet_b0\"), baseline_ckpt)\nav2       = load_best(RealVisionAV(backbone=\"tf_efficientnet_b0\", v_emb=256, a_emb=256, nheads=4, nlayers=2), av_ckpt)\n\nb_acc,b_f1,b_auc = eval_on_test(baseline2, test_loader, is_av=False)\na_acc,a_f1,a_auc = eval_on_test(av2, test_loader, is_av=True)\n\nprint(f\"Baseline  | ACC {b_acc:.3f}  F1 {b_f1:.3f}  AUC {b_auc:.3f}\")\nprint(f\"AV Model  | ACC {a_acc:.3f}  F1 {a_f1:.3f}  AUC {a_auc:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T13:57:38.917001Z","iopub.execute_input":"2026-02-15T13:57:38.917369Z","iopub.status.idle":"2026-02-15T14:02:59.178836Z","shell.execute_reply.started":"2026-02-15T13:57:38.917334Z","shell.execute_reply":"2026-02-15T14:02:59.178059Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Advanced Evaluation and Performance Analysis\n\n<p style=\"font-size:18px; font-weight:normal;\">\nThis section provides a comprehensive evaluation of both models using Confusion Matrices, ROC curves, Precision-Recall curves, and detailed classification reports.  \nThese visual and quantitative analyses offer deeper insight into class-wise performance, false positives/negatives, and the overall discriminative capability of each model.\n</p>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import (\n    confusion_matrix, ConfusionMatrixDisplay,\n    RocCurveDisplay, classification_report\n)\n\ndef collect_probs_and_labels(model, loader, is_av: bool):\n    model.eval()\n    all_y = []\n    all_prob1 = []  # prob of class 1 (fake)\n    with torch.no_grad():\n        for v, a, y in loader:\n            v = v.to(device, non_blocking=True)\n            a = a.to(device, non_blocking=True)\n            logits = model(v, a) if is_av else model(v)\n            prob = torch.softmax(logits, dim=1)[:, 1].detach().cpu().numpy()\n            all_prob1.append(prob)\n            all_y.append(y.numpy())\n    all_prob1 = np.concatenate(all_prob1, axis=0)\n    all_y = np.concatenate(all_y, axis=0)\n    return all_y, all_prob1\n\n# Collect test predictions\ny_b, p1_b = collect_probs_and_labels(baseline2, test_loader, is_av=False)\ny_a, p1_a = collect_probs_and_labels(av2,       test_loader, is_av=True)\n\n# Convert probs to predicted labels (threshold 0.5)\npred_b = (p1_b >= 0.5).astype(int)\npred_a = (p1_a >= 0.5).astype(int)\n\n# ---- Confusion Matrices ----\nplt.figure()\ncm_b = confusion_matrix(y_b, pred_b, labels=[0,1])\nConfusionMatrixDisplay(cm_b, display_labels=[\"real(0)\", \"fake(1)\"]).plot(values_format=\"d\")\nplt.title(\"Baseline - Confusion Matrix (Test)\")\nplt.show()\n\nplt.figure()\ncm_a = confusion_matrix(y_a, pred_a, labels=[0,1])\nConfusionMatrixDisplay(cm_a, display_labels=[\"real(0)\", \"fake(1)\"]).plot(values_format=\"d\")\nplt.title(\"AV Model - Confusion Matrix (Test)\")\nplt.show()\n\n# ---- ROC Curves ----\nplt.figure()\nRocCurveDisplay.from_predictions(y_b, p1_b, name=\"Baseline\")\nRocCurveDisplay.from_predictions(y_a, p1_a, name=\"AV Model\")\nplt.title(\"ROC Curves (Test)\")\nplt.show()\n\n# ---- Per-class report ----\nprint(\"Baseline - Classification Report (Test):\")\nprint(classification_report(y_b, pred_b, target_names=[\"real(0)\", \"fake(1)\"], digits=3))\n\nprint(\"AV Model - Classification Report (Test):\")\nprint(classification_report(y_a, pred_a, target_names=[\"real(0)\", \"fake(1)\"], digits=3))\n\nfrom sklearn.metrics import PrecisionRecallDisplay\n\nplt.figure()\nPrecisionRecallDisplay.from_predictions(y_b, p1_b, name=\"Baseline\")\nPrecisionRecallDisplay.from_predictions(y_a, p1_a, name=\"AV Model\")\nplt.title(\"Precision-Recall Curves (Test)\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T14:02:59.18012Z","iopub.execute_input":"2026-02-15T14:02:59.18046Z","iopub.status.idle":"2026-02-15T14:08:17.687479Z","shell.execute_reply.started":"2026-02-15T14:02:59.180426Z","shell.execute_reply":"2026-02-15T14:08:17.686812Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Explainability (XAI) – Grad-CAM Visualization\n\n<p style=\"font-size:18px; font-weight:normal;\">\nThis section applies Grad-CAM to the trained multi-modal model in order to visualize which spatial regions influenced the deepfake prediction.  \nThe heatmap highlights the most discriminative pixels, providing interpretability and validating that the model focuses on meaningful facial areas.\n</p>","metadata":{}},{"cell_type":"code","source":"print(\"Loading best AV model...\")\n\nav2 = load_best(\n    RealVisionAV(\n        backbone=\"tf_efficientnet_b0\",\n        v_emb=256,\n        a_emb=256,\n        nheads=4,\n        nlayers=2\n    ),\n    av_ckpt\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T14:08:17.688795Z","iopub.execute_input":"2026-02-15T14:08:17.689067Z","iopub.status.idle":"2026-02-15T14:08:18.120525Z","shell.execute_reply.started":"2026-02-15T14:08:17.689038Z","shell.execute_reply":"2026-02-15T14:08:18.119902Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n=== XAI (Grad-CAM) Setup ===\")\nfrom pytorch_grad_cam import GradCAM\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\n\ntarget_layers = [av2.cnn.conv_head] if hasattr(av2.cnn, \"conv_head\") else [list(av2.cnn.modules())[-1]]\n\nclass _Wrapper(nn.Module):\n    def __init__(self, model, audio_tensor, T):\n        super().__init__()\n        self.model = model\n        self.audio = audio_tensor\n        self.T = T\n    def forward(self, x):\n        vv = x.unsqueeze(1).repeat(1, self.T, 1, 1, 1)\n        return self.model(vv, self.audio)\n\ndef gradcam_for_sample(v, a, y, title_prefix=\"\"):\n    v_in = v.unsqueeze(0).to(device)\n    a_in = a.unsqueeze(0).to(device)\n\n    # predict\n    with torch.no_grad():\n        logits = av2(v_in, a_in)\n        prob = torch.softmax(logits, dim=1)[0].detach().cpu().numpy()\n        pred = int(np.argmax(prob))\n\n    # choose a frame\n    t_idx = min(5, v.shape[0] - 1)\n    frame = v_in[:, t_idx]  # (1,3,H,W)\n\n    wrapper = _Wrapper(av2, a_in, cfg.k_frames).to(device).eval()\n    cam = GradCAM(model=wrapper, target_layers=target_layers)\n\n    grayscale_cam = cam(input_tensor=frame, targets=None)[0]  # (H,W)\n\n    img = frame.detach().cpu().squeeze(0).permute(1,2,0).numpy()\n    img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n    vis = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n\n    plt.figure(figsize=(6,6))\n    plt.title(f\"{title_prefix} | True={y} Pred={pred} ProbFake={prob[1]:.3f}\")\n    plt.imshow(vis)\n    plt.axis(\"off\")\n    plt.show()\n\ndef find_example(test_ds, want_true, want_correct=True, max_tries=300):\n    for i in range(min(max_tries, len(test_ds))):\n        v, a, y = test_ds[i]\n        y = int(y.item()) if hasattr(y, \"item\") else int(y)\n\n        with torch.no_grad():\n            logits = av2(v.unsqueeze(0).to(device), a.unsqueeze(0).to(device))\n            pred = int(torch.argmax(logits, dim=1).item())\n\n        correct = (pred == y)\n        if (y == want_true) and (correct == want_correct):\n            return i, v, a, y, pred\n    return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T14:08:18.121396Z","iopub.execute_input":"2026-02-15T14:08:18.121663Z","iopub.status.idle":"2026-02-15T14:08:18.312431Z","shell.execute_reply.started":"2026-02-15T14:08:18.121635Z","shell.execute_reply":"2026-02-15T14:08:18.311671Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===== XAI (Grad-CAM): True Real (True=0, Pred=0) =====\n\nprint(\"\\n=== XAI: True REAL (True=0, Pred=0) ===\")\n\nfrom pytorch_grad_cam import GradCAM\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\n\n# --- safety checks ---\nassert \"av2\" in globals(), \"av2 לא מוגדר. תריצי קודם את התא של load_best + יצירת av2.\"\nassert \"test_ds\" in globals(), \"test_ds לא מוגדר. תריצי קודם את התא של הדאטהסט.\"\nassert \"cfg\" in globals(), \"cfg לא מוגדר. תריצי קודם את תא ה-Config.\"\nassert \"device\" in globals(), \"device לא מוגדר.\"\n\n# pick a reasonable target layer\ntarget_layers = [av2.cnn.conv_head] if hasattr(av2.cnn, \"conv_head\") else [list(av2.cnn.modules())[-1]]\n\nclass _Wrapper(nn.Module):\n    def __init__(self, model, audio_tensor, T):\n        super().__init__()\n        self.model = model\n        self.audio = audio_tensor\n        self.T = T\n    def forward(self, x):\n        vv = x.unsqueeze(1).repeat(1, self.T, 1, 1, 1)\n        return self.model(vv, self.audio)\n\n@torch.no_grad()\ndef predict_one(v, a):\n    v_in = v.unsqueeze(0).to(device)\n    a_in = a.unsqueeze(0).to(device)\n    logits = av2(v_in, a_in)\n    prob = torch.softmax(logits, dim=1)[0].detach().cpu().numpy()\n    pred = int(np.argmax(prob))\n    return pred, prob\n\ndef find_true_real_example(ds, max_tries=5000):\n    # want: True=0 (real) and Pred=0 (real)\n    for i in range(min(max_tries, len(ds))):\n        v, a, y = ds[i]\n        y_int = int(y.item()) if hasattr(y, \"item\") else int(y)\n        pred, prob = predict_one(v, a)\n        if y_int == 0 and pred == 0:\n            return i, v, a, y_int, pred, float(prob[1])\n    return None\n\ndef gradcam_show(v, a, y_true, pred, prob_fake, title_prefix=\"Grad-CAM (True Real)\"):\n    v_in = v.unsqueeze(0).to(device)\n    a_in = a.unsqueeze(0).to(device)\n\n    # choose a frame\n    t_idx = min(5, v.shape[0] - 1)\n    frame = v_in[:, t_idx]  # (1,3,H,W)\n\n    wrapper = _Wrapper(av2, a_in, cfg.k_frames).to(device).eval()\n    cam = GradCAM(model=wrapper, target_layers=target_layers)\n\n    grayscale_cam = cam(input_tensor=frame, targets=None)[0]  # (H,W)\n\n    img = frame.detach().cpu().squeeze(0).permute(1,2,0).numpy()\n    img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n    vis = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n\n    plt.figure(figsize=(6, 6))\n    plt.title(f\"{title_prefix} | True={y_true} Pred={pred} ProbFake={prob_fake:.3f}\")\n    plt.imshow(vis)\n    plt.axis(\"off\")\n    plt.show()\n\nres = find_true_real_example(test_ds, max_tries=5000)\nif res is None:\n    print(\"Could not find a True=0 & Pred=0 example in the first tries. נסי להגדיל max_tries או לבדוק אם המודל מנבא בעיקר 1.\")\nelse:\n    i, v, a, y_true, pred, prob_fake = res\n    print(f\"Using index: {i} | True={y_true} Pred={pred} ProbFake={prob_fake:.3f}\")\n    gradcam_show(v, a, y_true, pred, prob_fake, title_prefix=\"Grad-CAM (True Real)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T14:08:18.313509Z","iopub.execute_input":"2026-02-15T14:08:18.313969Z","iopub.status.idle":"2026-02-15T14:08:26.610307Z","shell.execute_reply.started":"2026-02-15T14:08:18.313946Z","shell.execute_reply":"2026-02-15T14:08:26.609761Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"res = find_example(test_ds, want_true=1, want_correct=True)\nif res is None:\n    print(\"Could not find a correct FAKE example in the first 300 samples.\")\nelse:\n    i, v, a, y, pred = res\n    print(\"Using index:\", i, \"| True:\", y, \"| Pred:\", pred)\n    gradcam_for_sample(v, a, y, title_prefix=\"Grad-CAM (True Fake)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T14:08:26.611078Z","iopub.execute_input":"2026-02-15T14:08:26.611347Z","iopub.status.idle":"2026-02-15T14:08:28.155795Z","shell.execute_reply.started":"2026-02-15T14:08:26.611314Z","shell.execute_reply":"2026-02-15T14:08:28.155063Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Try to find any misclassified example (either real->fake or fake->real)\nfound = None\nfor want_true in [0, 1]:\n    res = find_example(test_ds, want_true=want_true, want_correct=False)\n    if res is not None:\n        found = res\n        break\n\nif found is None:\n    print(\"Could not find a misclassified example in the first 300 samples.\")\nelse:\n    i, v, a, y, pred = found\n    print(\"Using index:\", i, \"| True:\", y, \"| Pred:\", pred)\n    gradcam_for_sample(v, a, y, title_prefix=\"Grad-CAM (Misclassified)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-15T14:08:28.1568Z","iopub.execute_input":"2026-02-15T14:08:28.157062Z","iopub.status.idle":"2026-02-15T14:08:31.620391Z","shell.execute_reply.started":"2026-02-15T14:08:28.157039Z","shell.execute_reply":"2026-02-15T14:08:31.619831Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  XAI Analysis – Grad-CAM Interpretation\n\n <p style=\"font-size:18px; font-weight:normal;\">\nThe Grad-CAM visualization highlights the spatial regions that most influenced the model’s prediction.In this example, the model focuses primarily on facial areas (eyes, nose, mouth, and skin boundaries), which are known to contain subtle inconsistencies in Deepfake manipulations.This supports the hypothesis that the network learns meaningful visual artifacts rather than relying on background or irrelevant cues, strengthening the interpretability and reliability of the proposed approach.\n </p>\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}